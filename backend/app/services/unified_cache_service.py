"""
Unified Cache Service —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ Cursor Prompt
–†–µ–∞–ª–∏–∑—É–µ—Ç —è–≤–Ω—ã–µ —Ä–µ–∂–∏–º—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è: default/bypass/refresh —Å –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å—é
"""

import json
import os
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, Tuple
from dataclasses import dataclass
import redis
from string import Template

logger = logging.getLogger(__name__)


class CacheMode:
    """–†–µ–∂–∏–º—ã –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏"""
    DEFAULT = "default"    # –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –µ—Å–ª–∏ –≤–∞–ª–∏–¥–µ–Ω
    BYPASS = "bypass"      # –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º –∫—ç—à, –Ω–µ —á–∏—Ç–∞–µ–º/–Ω–µ –ø–∏—à–µ–º
    REFRESH = "refresh"    # –ø–µ—Ä–µ—Å—á—ë—Ç –∏ –ø–µ—Ä–µ–∑–∞–ø–∏—Å—å –∫—ç—à–∞


class CacheState:
    """–°–æ—Å—Ç–æ—è–Ω–∏—è –∫—ç—à–∞ –¥–ª—è response headers"""
    HIT = "HIT"           # —Å–≤–µ–∂–∏–π –∫—ç—à –Ω–∞–π–¥–µ–Ω
    MISS = "MISS"         # –∫—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –∏—Å—Ç—ë–∫
    BYPASS = "BYPASS"     # –∫—ç—à –ø—Ä–æ–ø—É—â–µ–Ω
    REFRESH = "REFRESH"   # –∫—ç—à –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –æ–±–Ω–æ–≤–ª—ë–Ω
    STALE = "STALE"       # —É—Å—Ç–∞—Ä–µ–≤—à–∏–π –∫—ç—à (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)


@dataclass
class CacheMetadata:
    """–ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∫—ç—à–∞ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ API —Å SWR"""
    cached: bool                      # –∏–∑ –∫—ç—à–∞ –∏–ª–∏ —Å–≤–µ–∂–∏–π?
    cache_key: str                   # –∫–ª—é—á –∫—ç—à–∞
    model_name: str               # –∏–º—è –º–æ–¥–µ–ª–∏
    last_updated: datetime           # –∫–æ–≥–¥–∞ –æ–±–Ω–æ–≤–ª—ë–Ω
    compute_ms: int                  # –≤—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (LLM+postprocess)
    llm_latency_ms: int = 0         # –≤—Ä–µ–º—è —á–∏—Å—Ç–æ–≥–æ LLM –≤—ã–∑–æ–≤–∞
    
    # SWR –ø–æ–¥–¥–µ—Ä–∂–∫–∞
    cache_age: int = 0               # –≤–æ–∑—Ä–∞—Å—Ç –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
    etag: str = ""                   # ETag –¥–ª—è conditional requests
    is_stale: bool = False           # —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –∫—ç—à —É—Å—Ç–∞—Ä–µ–≤—à–∏–º
    can_stale_while_revalidate: bool = False  # –º–æ–∂–Ω–æ –ª–∏ –æ—Ç–¥–∞—Ç—å stale —Å —Ñ–æ–Ω–æ–≤—ã–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º


@dataclass
class UnifiedCacheConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å SWR –∏ Circuit Breaker"""
    # –ë–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    REDIS_HOST = 'localhost'
    REDIS_PORT = 6379
    REDIS_DB = 0
    
    # TTL —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ SWR
    DEFAULT_TTL = 24 * 60 * 60         # 24 —á–∞—Å–∞ –æ—Å–Ω–æ–≤–Ω–æ–π TTL  
    REFRESH_THRESHOLD = 15 * 60        # 15 –º–∏–Ω—É—Ç refresh threshold
    STALE_GRACE = 2 * 60 * 60          # 2 —á–∞—Å–∞ stale grace period
    
    # Circuit Breaker –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
    CIRCUIT_BREAKER_FAILURES = 3       # N=3 consecutive failures
    CIRCUIT_BREAKER_WINDOW = 5 * 60    # 5 –º–∏–Ω—É—Ç –æ–∫–Ω–æ
    CIRCUIT_BREAKER_RECOVERY = 10 * 60 # 10 –º–∏–Ω—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à
    
    # Schema –≤–µ—Ä—Å–∏–∏ –¥–ª—è –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏–∏
    SCHEMA_VERSION = "insights_v2"      # –≤–µ—Ä—Å–∏—è —Å—Ö–µ–º—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞  
    CIRCUIT_BREAKER_KEY = "insights:circuit_breaker_llm"
    
    # ETag –ø–æ–¥–¥–µ—Ä–∂–∫–∞
    ETAG_SEED = "insights-etag-v2"
    
    # Single-flight –∑–∞—â–∏—Ç–∞ –æ—Ç —à—Ç–æ—Ä–º–æ–≤
    SINGLE_FLIGHT_TTL = 120          # 2 –º–∏–Ω—É—Ç—ã –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞
    MAX_RETRIES = 3
    RETRY_DELAY = 1.0


class UnifiedCacheService:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏
    
    –†–µ–∞–ª–∏–∑—É–µ—Ç:
    - –†–µ–∂–∏–º—ã default/bypass/refresh
    - –î–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–ª—é—á–∏ –∫—ç—à–∞ —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤—Ö–æ–¥–∞
    - –ü—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å —á–µ—Ä–µ–∑ headers –∏ JSON –ø–æ–ª—è
    - –ú–µ—Ç—Ä–∏–∫–∏ –∑–∞–¥–µ—Ä–∂–∫–∏ (LLM + E2E)
    - –ó–∞—â–∏—Ç—É –æ—Ç —à—Ç–æ—Ä–º–æ–≤ —á–µ—Ä–µ–∑ single-flight
    """
    
    def __init__(self):
        # –ü–æ–ª—É—á–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        redis_host = os.getenv('REDIS_HOST', UnifiedCacheConfig.REDIS_HOST)
        redis_port = int(os.getenv('REDIS_PORT', UnifiedCacheConfig.REDIS_PORT))
        redis_db = int(os.getenv('REDIS_DB', UnifiedCacheConfig.REDIS_DB))
        
        self.redis_client = redis.Redis(
            host=redis_host,
            port=redis_port,
            db=redis_db,
            decode_responses=True
        )
        self.config = UnifiedCacheConfig()
    
    def normalize_inputs(
        self,
        prompt_data: Dict[str, Any],
        model_params: Dict[str, Any],
        request_params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–ª—é—á–∞ –∫—ç—à–∞
        
        Args:
            prompt_data: –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–º–ø—Ç–∞ (–ø–æ–∑–∏—Ü–∏–∏, –º–µ—Ç—Ä–∏–∫–∏)
            model_params: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ (model, temperature, top_p)
            request_params: –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ (horizon_months, risk_profile)
        
        Returns:
            –ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–æ–º–ø—Ç –¥–∞–Ω–Ω—ã–µ
        normalized_positions = [
            {
                "symbol": pos.get("symbol"),
                "quantity": pos.get("quantity"),
                "price": pos.get("price")  # –Ω–µ –≤–∫–ª—é—á–∞–µ–º timestamp —Ü–µ–Ω—ã
            }
            for pos in prompt_data.get("positions", [])
        ]
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ–∑–∏—Ü–∏–∏ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        normalized_positions.sort(key=lambda x: x["symbol"] or "")
        
        return {
            "positions": normalized_positions,
            "model": model_params.get("model"),
            "temperature": model_params.get("temperature", 0.2),
            "top_p": model_params.get("top_p", 1.0),
            "max_tokens": model_params.get("max_tokens", 400),
            "horizon_months": request_params.get("horizon_months"),
            "risk_profile": request_params.get("risk_profile"),
            "schema_version": self.config.SCHEMA_VERSION,
            # –ù–µ –≤–∫–ª—é—á–∞–µ–º timestamp –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
        }
    
    def create_cache_key(
        self,
        user_id: str,
        normalized_inputs: Dict[str, Any]
    ) -> str:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–ª—é—á–∞ –∫—ç—à–∞
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            normalized_inputs: –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        
        Returns:
            –ö–ª—é—á –∫—ç—à–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ: insights:user:{user_id}:{fingerprint_hash}
        """
        # –°–æ–∑–¥–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
        cache_input = json.dumps(normalized_inputs, sort_keys=True, separators=(',', ':'))
        
        # –ü—Ä–æ—Å—Ç–æ–µ —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–ª—é—á–∞
        import hashlib
        fingerprint = hashlib.md5(cache_input.encode()).hexdigest()
        
        return f"insights:unified:{user_id}:{fingerprint}"
    
    def get_cache_entry(
        self,
        cache_key: str,
        mode: str = CacheMode.DEFAULT
    ) -> Tuple[Optional[Dict], Optional[CacheMetadata], str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞ —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–∂–∏–º—É
        
        Args:
            cache_key: –∫–ª—é—á –∫—ç—à–∞
            mode: —Ä–µ–∂–∏–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è
        
        Returns:
            (data, metadata, state) –≥–¥–µ state –æ–¥–∏–Ω –∏–∑ CacheState
        """
        if mode == CacheMode.BYPASS:
            logger.info(f"Cache BYPASS for key {cache_key}")
            return None, None, CacheState.BYPASS
        
        try:
            cached_data = self.redis_client.get(cache_key)
            
            if cached_data:
                cache_payload = json.loads(cached_data)
                
                metadata = CacheMetadata(
                    cached=True,
                    cache_key=cache_key,
                    model_name=cache_payload.get("model_version", "unknown"),
                    last_updated=datetime.fromisoformat(cache_payload["last_updated"]),
                    compute_ms=cache_payload.get("compute_ms", 0),
                    llm_latency_ms=cache_payload.get("llm_latency_ms", 0)
                )
                
                if mode == CacheMode.REFRESH:
                    logger.info(f"Cache REFRESH requested for key {cache_key}")
                    return cache_payload["data"], metadata, CacheState.REFRESH
                else:
                    logger.info(f"Cache HIT for key {cache_key}")
                    return cache_payload["data"], metadata, CacheState.HIT
            
            else:
                logger.info(f"Cache MISS for key {cache_key}")
                return None, None, CacheState.MISS
                
        except Exception as e:
            logger.error(f"Cache read error for key {cache_key}: {e}")
            return None, None, CacheState.MISS
    
    def set_cache_entry(
        self,
        cache_key: str,
        data: Dict[str, Any],
        model_name: str,
        compute_ms: int,
        llm_latency_ms: int = 0,
        mode: str = CacheMode.DEFAULT
    ) -> None:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ –∫—ç—à —Å–æ–≥–ª–∞—Å–Ω–æ —Ä–µ–∂–∏–º—É
        
        Args:
            cache_key: –∫–ª—é—á –∫—ç—à–∞
            data: –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            model_name: –∏–º—è –º–æ–¥–µ–ª–∏
            compute_ms: –≤—Ä–µ–º—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
            llm_latency_ms: –≤—Ä–µ–º—è LLM –≤—ã–∑–æ–≤–∞
            mode: —Ä–µ–∂–∏–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è (bypass –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç)
        """
        if mode == CacheMode.BYPASS:
            logger.info(f"Cache BYPASS set for key {cache_key} - skipping save")
            return
        
        try:
            cache_payload = {
                "data": data,
                "model_version": model_name,
                "last_updated": datetime.utcnow().isoformat(),
                "compute_ms": compute_ms,
                "llm_latency_ms": llm_latency_ms,
                "schema_version": self.config.SCHEMA_VERSION,
            }
            
            self.redis_client.setex(
                cache_key,
                self.config.DEFAULT_TTL,
                json.dumps(cache_payload)
            )
            
            logger.info(f"Cached insights data for key {cache_key}")
            
        except Exception as e:
            logger.error(f"Cache write error for key {cache_key}: {e}")
    
    def acquire_computation_lock(self, cache_key: str) -> Tuple[bool, Optional[str]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ –ª–æ–∫–∞ –¥–ª—è single-flight
        
        Args:
            cache_key: –∫–ª—é—á –∫—ç—à–∞ –¥–ª—è –ª–æ–∫–∞
        
        Returns:
            (success, lock_key) - —É—Å–ø–µ—Ö –∏ –∫–ª—é—á –ª–æ–∫–∞
        """
        lock_key = f"lock:{cache_key}"
        
        try:
            if self.redis_client.setnx(lock_key, "locked"):
                self.redis_client.expire(lock_key, self.config.SINGLE_FLIGHT_TTL)
                logger.info(f"Acquired single-flight lock {lock_key}")
                return True, lock_key
            else:
                logger.info(f"Single-flight deduplication for {lock_key}")
                return False, lock_key
                
        except Exception as e:
            logger.error(f"Lock acquisition error for {lock_key}: {e}")
            return False, None
    
    def release_computation_lock(self, lock_key: str) -> None:
        """–û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ –ª–æ–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è"""
        try:
            self.redis_client.delete(lock_key)
            logger.info(f"Released single-flight lock {lock_key}")
        except Exception as e:
            logger.error(f"Lock release error for {lock_key}: {e}")
    
    def invalidate_user_cache(self, user_id: str) -> Dict[str, Any]:
        """
        –ò–Ω–≤–∞–ª–∏–¥–∞—Ü–∏—è –∫—ç—à–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Args:
            user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏–Ω–≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        try:
            pattern = f"insights:unified:{user_id}:*"
            keys = self.redis_client.keys(pattern)
            
            if keys:
                deleted_count = self.redis_client.delete(*keys)
                logger.info(f"Invalidated {deleted_count} cache entries for user {user_id}")
                return {
                    "invalidated_keys": deleted_count,
                    "user_id": user_id,
                    "type": "user_cache_clear"
                }
            else:
                return {
                    "invalidated_keys": 0,
                    "user_id": user_id,
                    "type": "no_entries_found"
                }
                
        except Exception as e:
            logger.error(f"Cache invalidation error for user {user_id}: {e}")
            return {
                "invalidated_keys": 0,
                "user_id": user_id,
                "type": "error",
                "error": str(e)
            }
    
    def get_cache_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫—ç—à–∞"""
        try:
            pattern = "insights:unified:*"
            keys = self.redis_client.keys(pattern)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
            user_counts = {}
            for key in keys:
                parts = key.split(':')
                if len(parts) >= 3:
                    user_id = parts[2]
                    user_counts[user_id] = user_counts.get(user_id, 0) + 1
            
            return {
                "total_entries": len(keys),
                "unique_users": len(user_counts),
                "user_breakdown": user_counts,
                "timestamp": datetime.utcnow().isoformat(),
                "estimated_memory_bytes": len(keys) * 1000,  # –ø—Ä–∏–º–µ—Ä–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            }
            
        except Exception as e:
            logger.error(f"Cache stats error: {e}")
            return {"error": str(e)}
    
    def log_cache_operation(
        self,
        operation: str,
        cache_key: str,
        cache_state: str,
        elapsed_ms: Optional[int] = None,
        llm_ms: Optional[int] = None,
        **kwargs
    ) -> None:
        """
        –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –∫—ç—à —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏
        
        Args:
            operation: —Ç–∏–ø –æ–ø–µ—Ä–∞—Ü–∏–∏ (get_insights, refresh_insights)
            cache_key: –∫–ª—é—á –∫—ç—à–∞ (—É—Å–µ—á—ë–Ω–Ω—ã–π –¥–ª—è –ª–æ–≥–æ–≤)
            cache_state: —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫—ç—à–∞ (HIT/MISS/BYPASS/REFRESH)
            elapsed_ms: –æ–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
            llm_ms: –≤—Ä–µ–º—è LLM –≤—ã–∑–æ–≤–∞
            **kwargs: –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        """
        truncated_key = cache_key[:16] + "..." if len(cache_key) > 16 else cache_key
        
        log_data = {
            "operation": operation,
            "cache_key": truncated_key,
            "cache_state": cache_state,
            "ttl": self.config.DEFAULT_TTL,
        }
        
        if elapsed_ms is not None:
            log_data["total_ms"] = elapsed_ms
        if llm_ms is not None:
            log_data["llm_ms"] = llm_ms
        
        log_data.update(kwargs)
        
        logger.info(f"Cache operation: {log_data}")
    
    def _compute_portfolio_hash(self, user_id: str, request_data: Dict[str, Any]) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ö–µ—à –ø–æ—Ä—Ç—Ñ–µ–ª—è –¥–ª—è –∫–ª—é—á–∞ –∫—ç—à–∞"""
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ–≥–æ —Ö–µ—à–∞
        normalized_data = {
            'user_id': user_id,
            'horizon_months': request_data.get('horizon_months', 6),
            'risk_profile': request_data.get('risk_profile', 'Balanced'),
            'model': request_data.get('model', 'llama3.1:8b'),
            'schema_version': self.config.SCHEMA_VERSION
        }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –∫–ª—é—á–∏ –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
        sorted_items = sorted(normalized_data.items())
        hash_input = ''.join(f"{k}={v}" for k, v in sorted_items)
        
        # –ü—Ä–æ—Å—Ç–æ–π —Ö–µ—à –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ (–≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ hashlib.sha256)
        return str(hash(hash_input))[:16]
    
    def _generate_etag(self, user_id: str, timestamp: str, content_hash: str) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç ETag –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–Ω—Ç–∞"""
        etag_input = f"{self.config.ETAG_SEED}:{user_id}:{timestamp}:{content_hash}"
        return str(hash(etag_input))[-16:]  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 16 —Å–∏–º–≤–æ–ª–æ–≤ —Ö–µ—à–∞
    
    def _check_etag_match(self, etag: str, if_none_match: Optional[str]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ ETag –¥–ª—è conditional requests (304)"""
        return if_none_match and etag in if_none_match.split(', ')
    
    def _update_circuit_breaker(self, success: bool) -> bool:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ circuit breaker. –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ circuit –æ—Ç–∫—Ä—ã—Ç (—Å–ª–µ–¥—É–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à)"""
        now = time.time()
        current_state = self.redis_client.hgetall(self.config.CIRCUIT_BREAKER_KEY)
        
        if success:
            # –°–±—Ä–æ—Å circuit breaker –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
            self.redis_client.delete(self.config.CIRCUIT_BREAKER_KEY)
            logger.info("Circuit breaker reset due to success")
            return False
        else:
            # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫
            failures = int(current_state.get('failures', 0)) + 1
            windowStart = float(current_state.get('window_start', now))
            
            # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–µ –æ–∫–Ω–æ –µ—Å–ª–∏ –ø—Ä–æ—à–ª–æ –≤—Ä–µ–º—è
            if now - windowStart > self.config.CIRCUIT_BREAKER_WINDOW:
                windowStart = now
                failures = 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
            self.redis_client.hset(self.config.CIRCUIT_BREAKER_KEY, mapping={
                'failures': failures,
                'window_start': windowStart,
                'last_failure': now
            })
            
            # Circuit –æ—Ç–∫—Ä—ã—Ç –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –æ—à–∏–±–æ–∫
            if failures >= self.config.CIRCUIT_BREAKER_FAILURES:
                self.redis_client.expire(self.config.CIRCUIT_BREAKER_KEY, self.config.CIRCUIT_BREAKER_RECOVERY)
                logger.warning(f"Circuit breaker OPEN after {failures} failures within {self.config.CIRCUIT_BREAKER_WINDOW}s")
                return True
            
            return False
    
    def _is_circuit_breaker_open(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –æ—Ç–∫—Ä—ã—Ç –ª–∏ circuit breaker"""
        try:
            circuit_state = self.redis_client.exists(self.config.CIRCUIT_BREAKER_KEY)
            return bool(circuit_state)
        except Exception as e:
            logger.error(f"Failed to check circuit breaker: {e}")
            return False
    
    def get_insights_with_swr(self, 
                             user_id: str,
                             request_data: Dict[str, Any],
                             if_none_match: Optional[str] = None) -> Tuple[Optional[CacheMetadata], Optional[Dict], bool]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ —Å –ø–æ–ª–Ω–æ–π –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π SWR (Stale-While-Revalidate)
        
        SWR –ø–æ–≤–µ–¥–µ–Ω–∏–µ:
        - FRESH (age < 15min): HIT, –æ—Ç–¥–∞–µ–º –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ, background=false
        - STALE (15min < age < 2h): STALE, –æ—Ç–¥–∞–µ–º –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ, background=true  
        - EXPIRED (age > 2h): MISS, –±–ª–æ–∫–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç–∞, –∂–¥—É fresh data
        
        Returns:
            (metadata, data, should_revalidate_background)
        """
        portfolio_hash = self._compute_portfolio_hash(user_id, request_data)
        cache_key = f"insights:{portfolio_hash}:{self.config.SCHEMA_VERSION}"
        
        try:
            cached_entry = self.redis_client.get(cache_key)
            
            if not cached_entry:
                logger.info(f"Cache MISS for key {cache_key[:20]}... (no entry)")
                return (None, None, False)
            
            cache_payload = json.loads(cached_entry)
            
            # –í—ã—á–∏—Å–ª—è–µ–º –≤–æ–∑—Ä–∞—Å—Ç –∫—ç—à–∞ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
            cache_timestamp = datetime.fromisoformat(cache_payload["last_updated"])
            cache_age_seconds = int((datetime.now() - cache_timestamp).total_seconds())
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º ETag –¥–ª—è 304 Not Modified
            content_hash = str(hash(str(cache_payload["data"])))[-8:]
            etag = self._generate_etag(user_id, cache_payload["last_updated"], content_hash)
            
            if self._check_etag_match(etag, if_none_match):
                logger.info(f"ETag match, returning 304 for {cache_key[:20]}...")
                return (CacheMetadata(
                    cached=True,
                    cache_key=cache_key,
                    model_name=cache_payload.get("model_version", "unknown"),
                    last_updated=cache_timestamp,
                    compute_ms=0,
                    cache_age=cache_age_seconds,
                    etag=etag,
                    is_stale=False,
                    can_stale_while_revalidate=False
                ), None, False)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º None data –¥–ª—è 304
            
            # üéØ SWR —Å–æ—Å—Ç–æ—è–Ω–∏–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç—É –∫—ç—à–∞
            
            # FRESH: –≤–æ–∑—Ä–∞—Å—Ç –º–µ–Ω—å—à–µ refresh threshold - –æ—Ç–¥–∞–µ–º –±–µ–∑ —Ñ–æ–Ω–∞
            if cache_age_seconds <= self.config.REFRESH_THRESHOLD:
                logger.info(f"Cache HIT (FRESH) for {cache_key[:20]}... (age: {cache_age_seconds}s)")
                metadata = CacheMetadata(
                    cached=True,
                    cache_key=cache_key,
                    model_name=cache_payload.get("model_version", "unknown"),
                    last_updated=cache_timestamp,
                    compute_ms=cache_payload.get("compute_ms", 0),
                    llm_latency_ms=cache_payload.get("llm_latency_ms", 0),
                    cache_age=cache_age_seconds,
                    etag=etag,
                    is_stale=False,
                    can_stale_while_revalidate=False
                )
                return (metadata, cache_payload["data"], False)
            
            # STALE: –≤–æ–∑—Ä–∞—Å—Ç –±–æ–ª—å—à–µ refresh –Ω–æ –º–µ–Ω—å—à–µ stale grace - SWR
            elif cache_age_seconds <= self.config.STALE_GRACE:
                logger.info(f"Cache STALE for {cache_key[:20]}... (age: {cache_age_seconds}s) - background refresh")
                metadata = CacheMetadata(
                    cached=True,
                    cache_key=cache_key,
                    model_name=cache_payload.get("model_version", "unknown"),
                    last_updated=cache_timestamp,
                    compute_ms=cache_payload.get("compute_ms", 0),
                    llm_latency_ms=cache_payload.get("llm_latency_ms", 0),
                    cache_age=cache_age_seconds,
                    etag=etag,
                    is_stale=True,
                    can_stale_while_revalidate=True
                )
                return (metadata, cache_payload["data"], True)  # üîÑ Background refresh!
            
            # EXPIRED: –≤–æ–∑—Ä–∞—Å—Ç –±–æ–ª—å—à–µ stale grace - —á–∏—Å—Ç—ã–π MISS 
            else:
                logger.info(f"Cache EXPIRED for {cache_key[:20]}... (age: {cache_age_seconds}s) - removing from cache")
                # –£–¥–∞–ª—è–µ–º expired –∑–∞–ø–∏—Å—å
                self.redis_client.delete(cache_key)
                return (None, None, False)
            
        except Exception as e:
            logger.error(f"Cache read error for key {cache_key[:20]}...: {e}")
            return (None, None, False)
    
    def save_insights_with_swr(self,
                               user_id: str,
                               request_data: Dict[str, Any],
                               data: Dict[str, Any],
                               compute_ms: int,
                               llm_latency_ms: int = 0) -> Tuple[str, str]:
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–Ω—Å–∞–π—Ç–æ–≤ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–ª—é—á–∞–º–∏ SWR
        
        Returns:
            (cache_key, etag)
        """
        portfolio_hash = self._compute_portfolio_hash(user_id, request_data)
        cache_key = f"insights:{portfolio_hash}:{self.config.SCHEMA_VERSION}"
        
        try:
            now = datetime.now()
            
            # –°–æ–∑–¥–∞–µ–º payload —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π
            cache_payload = {
                "data": data,
                "model_version": request_data.get("model", "unknown"),
                "last_updated": now.isoformat(),
                "compute_ms": compute_ms,
                "llm_latency_ms": llm_latency_ms,
                "schema_version": self.config.SCHEMA_VERSION,
                "portfolio_hash": portfolio_hash,
                "request_metadata": {
                    "user_id": user_id,
                    "horizon_months": request_data.get("horizon_months", 6),
                    "risk_profile": request_data.get("risk_profile", "Balanced")
                }
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –ø–æ–ª–Ω—ã–º TTL (24 —á–∞—Å–∞)
            self.redis_client.setex(
                cache_key,
                self.config.DEFAULT_TTL,
                json.dumps(cache_payload, default=str)
            )
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º ETag
            content_hash = str(hash(str(data)))[-8:]
            etag = self._generate_etag(user_id, now.isoformat(), content_hash)
            
            logger.info(f"Saved insights to cache: {cache_key[:20]}... (TTL: {self.config.DEFAULT_TTL}s)")
            
            return (cache_key, etag)
            
        except Exception as e:
            logger.error(f"Failed to save insights to cache: {e}")
            return ("", "")
    
    def schedule_background_refresh(self,
                                   user_id: str,
                                   request_data: Dict[str, Any],
                                   llm_callback) -> bool:
        """
        –ü–ª–∞–Ω–∏—Ä—É–µ—Ç —Ñ–æ–Ω–æ–≤–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è STALE –¥–∞–Ω–Ω—ã—Ö
        
        llm_callback: —Ñ—É–Ω–∫—Ü–∏—è –∫–æ—Ç–æ—Ä–∞—è –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫—ç—à–∞
        Returns:
            True –µ—Å–ª–∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ
        """
        portfolio_hash = self._compute_portfolio_hash(user_id, request_data)
        cache_key = f"insights:{portfolio_hash}:{self.config.SCHEMA_VERSION}"
        lock_key = f"{cache_key}:refresh_lock"
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —É–∂–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ
            if self.redis_client.exists(lock_key):
                logger.info(f"Background refresh already in progress for {cache_key[:20]}...")
                return True
            
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º lock –Ω–∞ 10 –º–∏–Ω—É—Ç (–≤—Ä–µ–º—è –Ω–∞ LLM –≥–µ–Ω–µ—Ä–∞—Ü–∏—é)
            if self.redis_client.setnx(lock_key, str(time.time())):
                self.redis_client.expire(lock_key, 600)  # 10 –º–∏–Ω—É—Ç lock
                
                # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã Celery/fastapi-background-tasks
                # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç–æ –ª–æ–≥–∏—Ä—É–µ–º —á—Ç–æ –ø–ª–∞–Ω–∏—Ä—É–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
                logger.warning(f"‚úèÔ∏è Background refresh QUEUED for {cache_key[:20]}... "
                             f"(Real implementation would call {llm_callback.__name__})")
                
                return True
            else:
                logger.info(f"Background refresh lock already exists for {cache_key[:20]}...")
                return True
                
        except Exception as e:
            logger.error(f"Failed to schedule background refresh: {e}")
            return False
