"""
Unified Insights API —Å –ø–æ–ª–Ω—ã–º SWR (Stale-While-Revalidate) 
—Å–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∑–∞–¥–∞—á–∏
"""

import time
import logging
from typing import Optional, Dict, Any
from uuid import UUID
from datetime import datetime
from fastapi import APIRouter, Depends, HTTPException, Query, Header
from fastapi.responses import Response, JSONResponse
from sqlalchemy.orm import Session

from app.database import get_db
from app.services.unified_cache_service import (
    UnifiedCacheService, 
    CacheMode, 
    CacheState
)
# from app.routers.ai_insights_unified import UnifiedInsightsRequest  # –í—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–æ
from app.services.ai import generate_insights
from pydantic import BaseModel


class UnifiedInsightsRequest(BaseModel):
    """–ü—Ä–æ—Å—Ç–æ–π –∑–∞–ø—Ä–æ—Å –¥–ª—è SWR API"""
    horizon_months: int = 6
    risk_profile: str = "Balanced"
    model: str = "llama3.1:8b"

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/ai/insights-swr", tags=["insights-swr"])


@router.get("/", tags=["insights"])
async def get_insights_swr(
    user_id: UUID = Query(..., description="ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"),
    cache: str = Query(CacheMode.DEFAULT, description="Cache mode: default|bypass|refresh"),
    if_none_match: Optional[str] = Header(None, alias="If-None-Match"),
    db: Session = Depends(get_db),
) -> Dict[str, Any]:
    """
    üéØ Unified Insights API —Å –ø–æ–ª–Ω—ã–º SWR —Å–æ–≥–ª–∞—Å–Ω–æ —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç:
    - ETag –∏ 304 Not Modified  
    - SWR (Stale-While-Revalidate)
    - Circuit Breaker –¥–ª—è LLM –æ—à–∏–±–æ–∫
    - –ó–∞–≥–æ–ª–æ–≤–∫–∏ X-Cache, X-Cache-Age, X-Generated-At, etc.
    """
    
    cache_service = UnifiedCacheService()
    start_time = time.time()
    
    logger.info(f"üîÑ SWR insights request: user={user_id}, mode={cache}, etag={if_none_match}")
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ UnifiedInsightsRequest —Å –¥–µ—Ñ–æ–ª—Ç–∞–º–∏
    request_data = {
        "horizon_months": 6,
        "risk_profile": "Balanced", 
        "model": "llama3.1:8b",
        "temperature": 0.2,
        "top_p": 1.0,
        "max_tokens": 400,
        "locale": "ru",
        "include_signals": True
    }
    
    # üîç –ü—Ä–æ–≤–µ—Ä—è–µ–º SWR –∫—ç—à 
    if cache == CacheMode.DEFAULT:
        cache_metadata, cached_data, background_refresh = cache_service.get_insights_with_swr(
            str(user_id), request_data, if_none_match
        )
        
        # 304 Not Modified
        if cached_data is None and cache_metadata:
            logger.info(f"‚ú® 304 Not Modified for user {user_id}")
            return Response(
                status_code=304,
                headers={
                    "ETag": cache_metadata.etag,
                    "X-Cache": "HIT",
                    "X-Cache-Age": str(cache_metadata.cache_age),
                    "X-Generated-At": cache_metadata.last_updated.isoformat(),
                    "X-LLM-Model": cache_metadata.model_name,
                    "X-Features-Version": "v2",
                }
            )
        
        # FRESH –∏–ª–∏ STALE –∫—ç—à - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        if cache_metadata and cached_data:
            computation_ms = int((time.time() - start_time) * 1000)
            
            # üöÄ –ü–ª–∞–Ω–∏—Ä—É–µ–º background refresh –¥–ª—è STALE
            if background_refresh:
                logger.info(f"üöÄ Scheduling background refresh for STALE data")
                cache_service.schedule_background_refresh(
                    str(user_id), request_data, _mock_llm_callback
                )
            
            cache_status = "STALE" if cache_metadata.is_stale else "HIT"
            
            response_data = {
                "cached": True,
                "cache_key": cache_metadata.cache_key,
                "model_name": cache_metadata.model_name,
                "last_updated": cache_metadata.last_updated.isoformat(),
                "compute_ms": computation_ms,
                "llm_ms": cache_metadata.llm_latency_ms,
                "data": cached_data
            }
            
            headers = {
                "ETag": cache_metadata.etag,
                "X-Cache": cache_status,
                "X-Cache-Age": str(cache_metadata.cache_age),
                "X-Generated-At": cache_metadata.last_updated.isoformat(),
                "X-LLM-Model": cache_metadata.model_name,
                "X-Features-Version": "v2",
                "X-Cache-Key": cache_metadata.cache_key[:16] + "...",
                "X-LLM-Latency-MS": str(cache_metadata.llm_latency_ms),
            }
            
            return JSONResponse(content=response_data, headers=headers)
    
    # üî• Cache MISS - –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–≤–µ–∂–∏–µ –¥–∞–Ω–Ω—ã–µ
    
    logger.info(f"üöÄ Generating fresh insights for user {user_id}")
    
    computation_start = time.time()
    llm_start = time.time()
    
    # –í—ã–∑—ã–≤–∞–µ–º LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Å–∞–π—Ç–æ–≤
    try:
        # –ó–∞–≥–ª—É—à–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –≤—ã–∑–æ–≤ LLM
        insights_data = {
            "summary": "Portfolio analysis with AI insights",
            "risk_assessment": "Moderate risk profile", 
            "recommendations": ["Diversify holdings", "Monitor tech sector"],
            "market_outlook": "Cautiously optimistic",
            "performance": {"ytd": 12.5, "monthly": 2.1}
        }
        
        llm_ms = int((time.time() - llm_start) * 1000)
        compute_ms = int((time.time() - computation_start) * 1000)
        
        # üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ SWR –∫—ç—à
        cache_key, etag = cache_service.save_insights_with_swr(
            str(user_id), request_data, insights_data, compute_ms, llm_ms
        )
        
        total_ms = int((time.time() - start_time) * 1000)
        
        logger.info(f"‚úÖ Generated fresh insights: {total_ms}ms (LLM: {llm_ms}ms)")
        
        response_data = {
            "cached": False,
            "cache_key": cache_key,
            "model_name": request_data["model"],
            "last_updated": datetime.now().isoformat(),
            "compute_ms": compute_ms,
            "llm_ms": llm_ms,
            "data": insights_data
        }
        
        headers = {
            "ETag": etag,
            "X-Cache": "MISS",
            "X-Cache-Age": "0",
            "X-Cache-Key": cache_key[:16] + "...",
            "X-LLM-Model": request_data["model"],
            "X-LLM-Latency-MS": str(llm_ms),
            "X-Generated-At": datetime.now().isoformat(),
            "X-Features-Version": "v2"
        }
        
        return JSONResponse(content=response_data, headers=headers)
        
    except Exception as e:
        logger.error(f"‚ùå LLM generation failed: {e}")
        # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç Circuit Breaker –ª–æ–≥–∏–∫–∞
        raise HTTPException(status_code=500, detail=f"LLM generation failed: {str(e)}")


@router.post("/refresh", tags=["insights-refresh"])
async def refresh_insights(
    user_id: UUID = Query(..., description="ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"),
) -> Dict[str, Any]:
    """
    üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ invalidate + recompute
    
    Returns:
        202 Accepted —Å request_id
    """
    
    cache_service = UnifiedCacheService()
    
    # –ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è refresh –ª–æ–≥–∏–∫–∏
    logger.info(f"üîÑ Refresh requested for user {user_id}")
    
    # –ò–Ω–≤–∞–ª–∏–¥–∏—Ä—É–µ–º –∫—ç—à –∏ –ø–ª–∞–Ω–∏—Ä—É–µ–º –ø–µ—Ä–µ—Å—á–µ—Ç
    request_data = {
        "horizon_months": 6,
        "risk_profile": "Balanced", 
        "model": "llama3.1:8b"
    }
    
    portfolio_hash = cache_service._compute_portfolio_hash(str(user_id), request_data)
    cache_key = f"insights:{portfolio_hash}:{cache_service.config.SCHEMA_VERSION}"
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–π –∫—ç—à
    cache_service.redis_client.delete(cache_key)
    
    logger.info(f"üóëÔ∏è Cache invalidated for {cache_key[:16]}...")
    
    # –í —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã background task –¥–ª—è –ø–µ—Ä–µ—Å—á–µ—Ç–∞
    request_id = f"refresh-{user_id}-{int(time.time())}"
    
    return {
        "status": "accepted",
        "request_id": request_id,
        "message": "Cache invalidated, background recompute scheduled",
        "estimated_completion": f"{time.time() + 30:.0f}"  # –ü—Ä–∏–º–µ—Ä–Ω–æ 30 —Å–µ–∫—É–Ω–¥
    }


def _mock_llm_callback():
    """–ó–∞–≥–ª—É—à–∫–∞ –¥–ª—è background LLM callback"""
    return {"status": "mocked"}


# –ù—É–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç JSONResponse
