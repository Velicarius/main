# Ollama - Локальные LLM для AI Portfolio Analyzer

## Что такое Ollama?

**Ollama** - это инструмент для запуска больших языковых моделей (LLM) локально на вашем компьютере. 
В отличие от облачных API (OpenAI, Anthropic), Ollama позволяет:
- Работать с моделями **офлайн** (без интернета после скачивания)
- **Контролировать данные** (не отправлять в облако)
- **Экономить деньги** (нет платы за токены)
- **Быстро тестировать** разные модели

## Ollama vs vLLM

| Ollama | vLLM |
|--------|------|
| ✅ **Простота**: `ollama run llama3.1:8b` | ❌ Сложная настройка |
| ✅ **Быстрый старт**: работает из коробки | ❌ Требует GPU, CUDA, Python |
| ✅ **Автоуправление**: сам скачивает модели | ❌ Ручная установка |
| ✅ **Кроссплатформа**: Windows, Mac, Linux | ❌ В основном Linux |
| ❌ Медленнее на GPU | ✅ **Высокая производительность** |
| ❌ Ограниченное масштабирование | ✅ **Масштабирование** для продакшена |

**Вывод**: Ollama для разработки и тестирования, vLLM для продакшена.

## Установка на Windows

### Вариант 1: Нативный Windows (рекомендуется)
```powershell
# Запустите install.ps1 из этой папки
.\install.ps1
```

### Вариант 2: WSL2 (если нужен Linux)
```bash
# В WSL2
curl -fsSL https://ollama.ai/install.sh | sh
```

## Зачем нужен сервис `ollama serve`?

**Ollama serve** - это HTTP API сервер, который:
- Слушает порт `11434` (http://localhost:11434)
- Принимает POST запросы к `/api/generate`
- Возвращает JSON с ответом модели
- **Без него** FastAPI не сможет общаться с моделями

```bash
# Запуск сервера (обязательно!)
ollama serve

# В другом терминале - тест
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "Привет! Как дела?"
}'
```

## Рекомендуемые модели для Portfolio Analyzer

### 1. **llama3.1:8b** (4.1GB)
- **Зачем**: Общие задачи анализа портфеля
- **Плюсы**: Хорошо понимает финансовые термины
- **Минусы**: Медленнее на CPU

### 2. **gemma2:9b** (5.4GB) 
- **Зачем**: Быстрые ответы, код-генерация
- **Плюсы**: Оптимизирована Google, быстрая
- **Минусы**: Меньше знаний о финансах

### 3. **qwen2.5-coder:7b** (4.4GB)
- **Зачем**: Генерация SQL, Python кода для анализа
- **Плюсы**: Специализирована на коде
- **Минусы**: Слабее в общих рассуждениях

## Быстрый старт

1. **Установите Ollama**: `.\install.ps1`
2. **Скачайте модели**: `.\pull_models.ps1`
3. **Запустите сервер**: `ollama serve`
4. **Тестируйте**: откройте `http://localhost:8000/llm_test.html`

## Troubleshooting

### Ollama не запускается
```bash
# Проверьте, что порт 11434 свободен
netstat -an | findstr 11434

# Перезапустите Ollama
ollama serve
```

### Модель не отвечает
```bash
# Проверьте, что модель скачана
ollama list

# Перескачайте модель
ollama pull llama3.1:8b
```

### Медленные ответы
- Используйте **квантованные** модели (q4_0, q5_0)
- Закройте другие приложения
- Рассмотрите vLLM для продакшена

